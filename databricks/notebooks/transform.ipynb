{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8a8b7f",
   "metadata": {},
   "source": [
    "### Bronze to Silver (Transform Bronze Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2e6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3be3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-NFAUVR6M:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BronzeToSilver</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x179a7fb6950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"BronzeToSilver\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a1c1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_lake_schema = '''\n",
    "  _row_id BIGINT,\n",
    "  txn_date STRING,\n",
    "  filler STRING,\n",
    "  txn_time STRING,\n",
    "  store_id STRING,\n",
    "  terminal_id STRING,\n",
    "  txn_id STRING,\n",
    "  cust_id STRING,\n",
    "  payment_mode STRING,\n",
    "  partner_bank STRING,\n",
    "  amount_paid DECIMAL(9,2),\n",
    "  bank_payable DECIMAL(9,2),\n",
    "  customer_payable DECIMAL(9,2),\n",
    "  currency_code STRING,\n",
    "  txn_status STRING\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01d84572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").schema(raw_data_lake_schema).option('header', True).load(r'D:\\mainframe_to_analytics_dev\\databricks\\catalog\\bronze\\BRONZE20260104.CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df9c9366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+---------------+-----------+------------+----------------+-------------+----------+\n",
      "|_row_id|  txn_date|filler|txn_time|store_id|terminal_id|      txn_id|   cust_id|payment_mode|   partner_bank|amount_paid|bank_payable|customer_payable|currency_code|txn_status|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+---------------+-----------+------------+----------------+-------------+----------+\n",
      "|      0|2026-01-04|      |12:47:16|  STR001|       T01 |TXN000001   |CUST89581 |  CASH      |NA             |    1988.85|     2038.57|         2018.68|          ESS|          |\n",
      "|      1|2026-01-04|      |12:47:16|  STR002|       T01 |TXN000002   |CUST62506 |  UPI       |HDFC           |     716.64|      734.56|          727.39|          ED |          |\n",
      "|      2|2026-01-04|      |12:47:16|  STR002|       T01 |TXN000003   |CUST55832 |  CARD      |HDFC           |    1379.19|     1413.67|         1399.88|          ED |          |\n",
      "|      3|2026-01-04|      |12:47:16|  STR001|       T01 |TXN000004   |CUST61397 |  CARD      |SBI            |    1812.28|     1857.59|         1839.46|          ESS|          |\n",
      "|      4|2026-01-04|      |12:47:16|  STR001|       T01 |TXN000005   |CUST68294 |  CARD      |HDFC           |    1135.01|     1163.39|         1152.04|          ESS|          |\n",
      "|      5|2026-01-04|      |12:47:16|  STR002|       T02 |TXN000006   |CUST89241 |  UPI       |HDFC           |    1423.33|     1458.91|         1444.68|          ESS|          |\n",
      "|      6|2026-01-04|      |12:47:16|  STR002|       T03 |TXN000007   |CUST18523 |  CASH      |NA             |     319.67|      327.66|          324.47|          ED |          |\n",
      "|      7|2026-01-04|      |12:47:16|  STR002|       T01 |TXN000008   |CUST28521 |  UPI       |NA             |    1024.26|     1049.87|         1039.62|          ESS|          |\n",
      "|      8|2026-01-04|      |12:47:16|  STR001|       T01 |TXN000009   |CUST24530 |  UPI       |ICICI          |     848.22|      869.43|          860.94|          ESS|          |\n",
      "|      9|2026-01-04|      |12:47:16|  STR001|       T02 |TXN000010   |CUST89502 |  UPI       |HDFC           |    1708.27|     1750.98|         1733.89|          ESS|          |\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+---------------+-----------+------------+----------------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24cb9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _row_id: long (nullable = true)\n",
      " |-- txn_date: string (nullable = true)\n",
      " |-- filler: string (nullable = true)\n",
      " |-- txn_time: string (nullable = true)\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- terminal_id: string (nullable = true)\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- payment_mode: string (nullable = true)\n",
      " |-- partner_bank: string (nullable = true)\n",
      " |-- amount_paid: decimal(9,2) (nullable = true)\n",
      " |-- bank_payable: decimal(9,2) (nullable = true)\n",
      " |-- customer_payable: decimal(9,2) (nullable = true)\n",
      " |-- currency_code: string (nullable = true)\n",
      " |-- txn_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3670cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Type Casting\n",
    "\n",
    "# txn_date, txn_time -> datetime, timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66b00380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, concat_ws, col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"txn_timestamp\",\n",
    "    to_timestamp(\n",
    "        concat_ws(\" \", col(\"txn_date\"), col(\"txn_time\")),\n",
    "        \"yyyy-MM-dd HH:mm:ss\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59768259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+---------------+-----------+------------+----------------+-------------+----------+-------------------+\n",
      "|_row_id|  txn_date|filler|txn_time|store_id|terminal_id|      txn_id|   cust_id|payment_mode|   partner_bank|amount_paid|bank_payable|customer_payable|currency_code|txn_status|      txn_timestamp|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+---------------+-----------+------------+----------------+-------------+----------+-------------------+\n",
      "|      0|2026-01-04|      |12:47:16|  STR001|       T01 |TXN000001   |CUST89581 |  CASH      |NA             |    1988.85|     2038.57|         2018.68|          ESS|          |2026-01-04 12:47:16|\n",
      "|      1|2026-01-04|      |12:47:16|  STR002|       T01 |TXN000002   |CUST62506 |  UPI       |HDFC           |     716.64|      734.56|          727.39|          ED |          |2026-01-04 12:47:16|\n",
      "|      2|2026-01-04|      |12:47:16|  STR002|       T01 |TXN000003   |CUST55832 |  CARD      |HDFC           |    1379.19|     1413.67|         1399.88|          ED |          |2026-01-04 12:47:16|\n",
      "|      3|2026-01-04|      |12:47:16|  STR001|       T01 |TXN000004   |CUST61397 |  CARD      |SBI            |    1812.28|     1857.59|         1839.46|          ESS|          |2026-01-04 12:47:16|\n",
      "|      4|2026-01-04|      |12:47:16|  STR001|       T01 |TXN000005   |CUST68294 |  CARD      |HDFC           |    1135.01|     1163.39|         1152.04|          ESS|          |2026-01-04 12:47:16|\n",
      "|      5|2026-01-04|      |12:47:16|  STR002|       T02 |TXN000006   |CUST89241 |  UPI       |HDFC           |    1423.33|     1458.91|         1444.68|          ESS|          |2026-01-04 12:47:16|\n",
      "|      6|2026-01-04|      |12:47:16|  STR002|       T03 |TXN000007   |CUST18523 |  CASH      |NA             |     319.67|      327.66|          324.47|          ED |          |2026-01-04 12:47:16|\n",
      "|      7|2026-01-04|      |12:47:16|  STR002|       T01 |TXN000008   |CUST28521 |  UPI       |NA             |    1024.26|     1049.87|         1039.62|          ESS|          |2026-01-04 12:47:16|\n",
      "|      8|2026-01-04|      |12:47:16|  STR001|       T01 |TXN000009   |CUST24530 |  UPI       |ICICI          |     848.22|      869.43|          860.94|          ESS|          |2026-01-04 12:47:16|\n",
      "|      9|2026-01-04|      |12:47:16|  STR001|       T02 |TXN000010   |CUST89502 |  UPI       |HDFC           |    1708.27|     1750.98|         1733.89|          ESS|          |2026-01-04 12:47:16|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+---------------+-----------+------------+----------------+-------------+----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c4f7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize values\n",
    "\n",
    "## eg 'UPI   ' to 'UPI' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a28e77fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "|_row_id|  txn_date|filler|txn_time|store_id|terminal_id|      txn_id|   cust_id|payment_mode|partner_bank|amount_paid|bank_payable|customer_payable|currency_code|txn_status|      txn_timestamp|IS_CASH|is_self_kiosk|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "|      0|2026-01-04|      |12:47:16|  STR001|        T01|TXN000001   |CUST89581 |        CASH|          NA|    1988.85|     2038.57|         2018.68|          ESS|          |2026-01-04 12:47:16|      Y|            Y|\n",
      "|      1|2026-01-04|      |12:47:16|  STR002|        T01|TXN000002   |CUST62506 |         UPI|        HDFC|     716.64|      734.56|          727.39|           ED|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      2|2026-01-04|      |12:47:16|  STR002|        T01|TXN000003   |CUST55832 |        CARD|        HDFC|    1379.19|     1413.67|         1399.88|           ED|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      3|2026-01-04|      |12:47:16|  STR001|        T01|TXN000004   |CUST61397 |        CARD|         SBI|    1812.28|     1857.59|         1839.46|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      4|2026-01-04|      |12:47:16|  STR001|        T01|TXN000005   |CUST68294 |        CARD|        HDFC|    1135.01|     1163.39|         1152.04|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      5|2026-01-04|      |12:47:16|  STR002|        T02|TXN000006   |CUST89241 |         UPI|        HDFC|    1423.33|     1458.91|         1444.68|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      6|2026-01-04|      |12:47:16|  STR002|        T03|TXN000007   |CUST18523 |        CASH|          NA|     319.67|      327.66|          324.47|           ED|          |2026-01-04 12:47:16|      Y|            Y|\n",
      "|      7|2026-01-04|      |12:47:16|  STR002|        T01|TXN000008   |CUST28521 |         UPI|          NA|    1024.26|     1049.87|         1039.62|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      8|2026-01-04|      |12:47:16|  STR001|        T01|TXN000009   |CUST24530 |         UPI|       ICICI|     848.22|      869.43|          860.94|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      9|2026-01-04|      |12:47:16|  STR001|        T02|TXN000010   |CUST89502 |         UPI|        HDFC|    1708.27|     1750.98|         1733.89|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim, col\n",
    "\n",
    "df = df.withColumn('payment_mode', trim(col('payment_mode')))\n",
    "df = df.withColumn('partner_bank', trim(col('partner_bank')))\n",
    "df = df.withColumn('currency_code', trim(col('currency_code')))\n",
    "df = df.withColumn('terminal_id', trim(col('terminal_id')))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8560f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('IS_CASH', when(((col('payment_mode') == 'UPI') | (col('payment_mode') == 'CARD')),'N').otherwise('Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fbd6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "|_row_id|  txn_date|filler|txn_time|store_id|terminal_id|      txn_id|   cust_id|payment_mode|partner_bank|amount_paid|bank_payable|customer_payable|currency_code|txn_status|      txn_timestamp|IS_CASH|is_self_kiosk|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "|      0|2026-01-04|      |12:47:16|  STR001|        T01|TXN000001   |CUST89581 |        CASH|          NA|    1988.85|     2038.57|         2018.68|          ESS|          |2026-01-04 12:47:16|      Y|            Y|\n",
      "|      1|2026-01-04|      |12:47:16|  STR002|        T01|TXN000002   |CUST62506 |         UPI|        HDFC|     716.64|      734.56|          727.39|           ED|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      2|2026-01-04|      |12:47:16|  STR002|        T01|TXN000003   |CUST55832 |        CARD|        HDFC|    1379.19|     1413.67|         1399.88|           ED|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      3|2026-01-04|      |12:47:16|  STR001|        T01|TXN000004   |CUST61397 |        CARD|         SBI|    1812.28|     1857.59|         1839.46|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      4|2026-01-04|      |12:47:16|  STR001|        T01|TXN000005   |CUST68294 |        CARD|        HDFC|    1135.01|     1163.39|         1152.04|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      5|2026-01-04|      |12:47:16|  STR002|        T02|TXN000006   |CUST89241 |         UPI|        HDFC|    1423.33|     1458.91|         1444.68|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      6|2026-01-04|      |12:47:16|  STR002|        T03|TXN000007   |CUST18523 |        CASH|          NA|     319.67|      327.66|          324.47|           ED|          |2026-01-04 12:47:16|      Y|            Y|\n",
      "|      7|2026-01-04|      |12:47:16|  STR002|        T01|TXN000008   |CUST28521 |         UPI|          NA|    1024.26|     1049.87|         1039.62|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      8|2026-01-04|      |12:47:16|  STR001|        T01|TXN000009   |CUST24530 |         UPI|       ICICI|     848.22|      869.43|          860.94|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "|      9|2026-01-04|      |12:47:16|  STR001|        T02|TXN000010   |CUST89502 |         UPI|        HDFC|    1708.27|     1750.98|         1733.89|          ESS|          |2026-01-04 12:47:16|      N|            Y|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79c9f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"is_self_kiosk\",\n",
    "    when(col(\"terminal_id\").isin(\"T01\", \"T02\"), \"N\")\n",
    "    .otherwise(\"Y\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ffe6483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "|_row_id|  txn_date|filler|txn_time|store_id|terminal_id|      txn_id|   cust_id|payment_mode|partner_bank|amount_paid|bank_payable|customer_payable|currency_code|txn_status|      txn_timestamp|IS_CASH|is_self_kiosk|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "|      0|2026-01-04|      |12:47:16|  STR001|        T01|TXN000001   |CUST89581 |        CASH|          NA|    1988.85|     2038.57|         2018.68|          ESS|          |2026-01-04 12:47:16|      Y|            N|\n",
      "|      1|2026-01-04|      |12:47:16|  STR002|        T01|TXN000002   |CUST62506 |         UPI|        HDFC|     716.64|      734.56|          727.39|           ED|          |2026-01-04 12:47:16|      N|            N|\n",
      "|      2|2026-01-04|      |12:47:16|  STR002|        T01|TXN000003   |CUST55832 |        CARD|        HDFC|    1379.19|     1413.67|         1399.88|           ED|          |2026-01-04 12:47:16|      N|            N|\n",
      "|      3|2026-01-04|      |12:47:16|  STR001|        T01|TXN000004   |CUST61397 |        CARD|         SBI|    1812.28|     1857.59|         1839.46|          ESS|          |2026-01-04 12:47:16|      N|            N|\n",
      "|      4|2026-01-04|      |12:47:16|  STR001|        T01|TXN000005   |CUST68294 |        CARD|        HDFC|    1135.01|     1163.39|         1152.04|          ESS|          |2026-01-04 12:47:16|      N|            N|\n",
      "|      5|2026-01-04|      |12:47:16|  STR002|        T02|TXN000006   |CUST89241 |         UPI|        HDFC|    1423.33|     1458.91|         1444.68|          ESS|          |2026-01-04 12:47:16|      N|            N|\n",
      "|      6|2026-01-04|      |12:47:16|  STR002|        T03|TXN000007   |CUST18523 |        CASH|          NA|     319.67|      327.66|          324.47|           ED|          |2026-01-04 12:47:16|      Y|            Y|\n",
      "|      7|2026-01-04|      |12:47:16|  STR002|        T01|TXN000008   |CUST28521 |         UPI|          NA|    1024.26|     1049.87|         1039.62|          ESS|          |2026-01-04 12:47:16|      N|            N|\n",
      "|      8|2026-01-04|      |12:47:16|  STR001|        T01|TXN000009   |CUST24530 |         UPI|       ICICI|     848.22|      869.43|          860.94|          ESS|          |2026-01-04 12:47:16|      N|            N|\n",
      "|      9|2026-01-04|      |12:47:16|  STR001|        T02|TXN000010   |CUST89502 |         UPI|        HDFC|    1708.27|     1750.98|         1733.89|          ESS|          |2026-01-04 12:47:16|      N|            N|\n",
      "+-------+----------+------+--------+--------+-----------+------------+----------+------------+------------+-----------+------------+----------------+-------------+----------+-------------------+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "355d8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e9a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o331.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from datetime import datetime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# SILVER_DIR = r\"D:/mainframe_to_analytics_dev/databricks/catalog/silver\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# run_date = datetime.now().strftime(\"%Y%m%d\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# SILVER_PATH = f\"{SILVER_DIR}/run_date={run_date}\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/mainframe_to_analytics_dev/databricks/catalog/silver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anzar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anzar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\anzar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\anzar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o331.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "## STEP 3 : BRONZE CSV FILE -> CATALOG\n",
    "\n",
    "## WILL WORK ON DATABRICKS\n",
    "import os\n",
    "from datetime import datetime\n",
    "SILVER_DIR = r\"D:\\mainframe_to_analytics_dev\\databricks\\catalog\\silver\"\n",
    "run_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "BRONZE_FILE = os.path.join(\n",
    "            SILVER_DIR, f\"BRONZE{run_date}.CSV\"\n",
    "        )\n",
    "\n",
    "os.makedirs(SILVER_DIR, exist_ok=True)\n",
    "\n",
    "silver_df.to_csv(BRONZE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb04d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
